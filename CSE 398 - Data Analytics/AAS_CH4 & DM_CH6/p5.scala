//--Random Decision Tree Forests--
/*
The reason our outputs dont match with book is due to the randomness. Cant possibly explore every decision rule at every level. 
DTrees use several heuristics to decide; only a few features are randomly analyzed at time (new tree er time); trade a bit of accuracy for speed 
THE RANDOMNESS IN PPROCESS creates independence between each tree (how many taxis are there and the group effect)
Trees that are autogenerated to fit noise (OVERFITTING) will be on average voted out. 
Making forest is easy, just change classifier
*/
/*HERE IS THE NEWWWWWWWWWW CODE*/
import org.apache.spark.ml.classification.RandomForestClassifier
val classifier = new RandomForestClassifier().
  setSeed(Random.nextLong()).
  setLabelCol("Cover_Type").
  setFeaturesCol("indexedVector").
  setPredictionCol("prediction")
import org.apache.spark.ml.Pipeline

//-The code below is to test the resultin decision tree forests-
import org.apache.spark.ml.feature.VectorIndexer
val inputCols = unencTrainData.columns.filter(_ != "Cover_Type") 	//all columns except the target are input features
val assembler = new VectorAssembler().
	setInputCols(inputCols).
	setOutputCol("featureVector")
val indexer = new VectorIndexer().
	setMaxCategories(40). //set to 40 because soil has 40 values
	setInputCol("featureVector").
	setOutputCol("indexedVector")
//this works because all values form 0-40 is present, but in scarce data set, may need to use VectorIndexerModel to manually map
val pipeline = new Pipeline().setStages(Array(assembler, indexer, classifier))


//-BELOW THIS IS TO TEST THE PIPELINE-
import org.apache.spark.ml.tuning.ParamGridBuilder	//to test hyperparameters
val paramGrid = new ParamGridBuilder().
	addGrid(classifier.impurity, Seq("entropy")).
	addGrid(classifier.maxDepth, Seq(20)).
	addGrid(classifier.maxBins, Seq(40)).	
	addGrid(classifier.minInfoGain, Seq(0.0)).
	build()
val multiclassEval = new MulticlassClassificationEvaluator().
	setLabelCol("Cover_Type").
	setPredictionCol("prediction").
	setMetricName("accuracy")	//kinda self explanotary what happening here
	
import org.apache.spark.ml.tuning.TrainValidationSplit	//can use CrossValidator here, but will coost k times more expensive & doesnt add much
val validator = new TrainValidationSplit().
	setSeed(Random.nextLong()).
	setEstimator(pipeline).
	setEvaluator(multiclassEval).
	setEstimatorParamMaps(paramGrid). //HYPER PARAMETERS CAN STILL OVER FIT
	setTrainRatio(0.9)	//take another 10 percent and set it aside
//the left out 10% is used as a crossvalidation set (evaluate parameters that fit to training set)
//the original 10% left out to evaluate hyperparameters that fit the CV^^ (examples that arent in CV but has not been trained on [real world data])


val validatorModel = validator.fit(unencTrainData)	//returns best overall pipeline
val bestModel = validatorModel.bestModel
println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)
val testAccuracy = multiclassEval.evaluate(bestModel.transform(unencTestData))
/*
bestModel: org.apache.spark.ml.Model[_] = pipeline_e8dfaf103521
{
        rfc_afd6081cae0e-cacheNodeIds: false,
        rfc_afd6081cae0e-checkpointInterval: 10,
        rfc_afd6081cae0e-featureSubsetStrategy: auto,
        rfc_afd6081cae0e-featuresCol: indexedVector,
        rfc_afd6081cae0e-impurity: entropy,
        rfc_afd6081cae0e-labelCol: Cover_Type,
        rfc_afd6081cae0e-maxBins: 40,
        rfc_afd6081cae0e-maxDepth: 20,
        rfc_afd6081cae0e-maxMemoryInMB: 256,
        rfc_afd6081cae0e-minInfoGain: 0.0,
        rfc_afd6081cae0e-minInstancesPerNode: 1,
        rfc_afd6081cae0e-numTrees: 20,
        rfc_afd6081cae0e-predictionCol: prediction,
        rfc_afd6081cae0e-probabilityCol: probability,
        rfc_afd6081cae0e-rawPredictionCol: rawPrediction,
        rfc_afd6081cae0e-seed: 8386058293792309184,
        rfc_afd6081cae0e-subsamplingRate: 1.0
}
testAccuracy: Double = 0.9522107081174439	//THATS A REAAALY GOOD ACCURACY!!!
*/
/*
I ran the code with training data and got:
val trainAccuracy = multiclassEval.evaluate(bestModel.transform(unencTrainData))
trainAccuracy: Double = 0.983749177996299
The model is overfitting still. As mentioned before, might be useful to decrease the length of the depth
*/


import org.apache.spark.ml.classification.RandomForestClassificationModel
val forestModel = bestModel.asInstanceOf[PipelineModel].
	stages.last.asInstanceOf[RandomForestClassificationModel]
forestModel.featureImportances.toArray.zip(inputCols).
	sorted.reverse.foreach(println)
/*
(0.2926106864847985,Elevation)
(0.18835406389537518,soil)
(0.10707829602517423,Horizontal_Distance_To_Roadways)
(0.10122135445840696,Horizontal_Distance_To_Fire_Points)
(0.06818651488923709,wilderness)
(0.05168345030406425,Horizontal_Distance_To_Hydrology)
(0.04499563182149587,Vertical_Distance_To_Hydrology)
(0.03328427707076898,Hillshade_Noon)
(0.03107486265165606,Aspect)
(0.030510493101422835,Hillshade_9am)
(0.02618167199051451,Hillshade_3pm)
(0.024818697307085434,Slope)
*/

//--Making Predictions--
bestModel.transform(unencTestData.drop("Cover_Type")).select("prediction").show()) //PREDICTED VALUES
/*
+----------+
|prediction|
+----------+
|       6.0|
|       6.0|
|       6.0|	
|       6.0|	
|       6.0|	
|       3.0|	
|       6.0|	
|       6.0|
|       3.0|
|       3.0|	//wrong
|       6.0|
|       4.0|
|       6.0|
|       3.0|	//wrong
|       4.0|
|       3.0|
|       3.0|
|       4.0|	
|       6.0|	
|       6.0|	//wrong
+----------+
*/
//out of the 20 shown, 17 was correct (0.85)

testData.select("Cover_Type").show()	//ACTUAL VALUES
/*
+----------+
|Cover_Type|
+----------+
|       6.0|
|       6.0|
|       6.0|
|       6.0|
|       6.0|
|       3.0|
|       6.0|
|       6.0|
|       3.0|
|       6.0|
|       6.0|
|       4.0|
|       6.0|
|       4.0|
|       4.0|
|       3.0|
|       3.0|
|       4.0|
|       6.0|
|       4.0|	
+----------+
*/
