//--Visualization with SparkR--
clusters_data <- read.df("/path/to/kddcup.data", "csv", inferSchema = "true", header = "false")
colnames(clusters_data) <- c(
  "duration", "protocol_type", "service", "flag",
  "src_bytes", "dst_bytes", "land", "wrong_fragment", "urgent",
  "hot", "num_failed_logins", "logged_in", "num_compromised",
  "root_shell", "su_attempted", "num_root", "num_file_creations",
  "num_shells", "num_access_files", "num_outbound_cmds",
  "is_host_login", "is_guest_login", "count", "srv_count",
  "serror_rate", "srv_serror_rate", "rerror_rate", "srv_rerror_rate",
  "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate",
  "dst_host_count", "dst_host_srv_count",
  "dst_host_same_srv_rate", "dst_host_diff_srv_rate",
  "dst_host_same_src_port_rate", "dst_host_srv_diff_host_rate",
  "dst_host_serror_rate", "dst_host_srv_serror_rate",
  "dst_host_rerror_rate", "dst_host_srv_rerror_rate",
  "label")
numeric_only <- cache(drop(clusters_data, c("protocol_type", "service", "flag", "label")))	//drop nonnumeric values

kmeans_model <- spark.kmeans(numeric_only, ~ ., k = 100, maxIter = 40, initMode = "k-means||") //~ means all columns

/*
From here, it’s straightforward to assign a cluster to each data point. The operations above show usage of the SparkR APIs, which naturally 
correspond to core Spark APIs but are expressed as R libraries in R-like syntax. The actual clustering is executed using the same JVM-based, 
Scala-language implementation in MLlib. These operations are effectively a handle or remote control to distributed operations that are not 
executing in R.
*/

clustering <- predict(kmeans_model, numeric_only)
clustering_sample <- collect(sample(clustering, FALSE, 0.01))
str(clustering_sample)
/*
'data.frame':   48990 obs. of  39 variables:
 $ duration                   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ src_bytes                  : int  212 212 217 234 239 236 239 307 212 230 ...
 $ dst_bytes                  : int  1402 1444 63479 1515 2164 2112 2112 468 1247 2395 ...
 $ land                       : int  0 0 0 0 0 0 0 0 0 0 ...
 $ wrong_fragment             : int  0 0 0 0 0 0 0 0 0 0 ...
 $ urgent                     : int  0 0 0 0 0 0 0 0 0 0 ...
 $ hot                        : int  0 0 0 0 0 0 0 0 0 0 ...
 $ num_failed_logins          : int  0 0 0 0 0 0 0 0 0 0 ...
 $ logged_in                  : int  1 1 1 1 1 1 1 1 1 1 ...
 $ num_compromised            : int  0 0 0 0 0 0 0 0 0 0 ...
 $ root_shell                 : int  0 0 0 0 0 0 0 0 0 0 ...
 $ su_attempted               : int  0 0 0 0 0 0 0 0 0 0 ...
 $ num_root                   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ num_file_creations         : int  0 0 0 0 0 0 0 0 0 0 ...
 $ num_shells                 : int  0 0 0 0 0 0 0 0 0 0 ...
 $ num_access_files           : int  0 0 0 0 0 0 0 0 0 0 ...
 $ num_outbound_cmds          : int  0 0 0 0 0 0 0 0 0 0 ...
 $ is_host_login              : int  0 0 0 0 0 0 0 0 0 0 ...
 $ is_guest_login             : int  0 0 0 0 0 0 0 0 0 0 ...
 $ count                      : int  4 12 2 8 5 18 30 8 19 14 ...
 $ srv_count                  : int  4 12 2 8 5 20 31 8 19 18 ...
 $ serror_rate                : num  0 0 0 0 0 0 0 0 0 0 ...
 $ srv_serror_rate            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ rerror_rate                : num  0 0 0 0 0 0 0 0 0 0 ...
 $ srv_rerror_rate            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ same_srv_rate              : num  1 1 1 1 1 1 1 1 1 1 ...
 $ diff_srv_rate              : num  0 0 0 0 0 0 0 0 0 0 ...
 $ srv_diff_host_rate         : num  0 0 0 0 0 0.1 0.06 0 0 0.11 ...
 $ dst_host_count             : int  4 12 2 8 40 53 174 242 255 255 ...
 $ dst_host_srv_count         : int  95 103 255 255 255 255 255 255 255 255 ...
 $ dst_host_same_srv_rate     : num  1 1 1 1 1 1 1 1 1 1 ...
 $ dst_host_diff_srv_rate     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ dst_host_same_src_port_rate: num  0.25 0.08 0.5 0.12 0.03 0.02 0.01 0 0 0 ...
 $ dst_host_srv_diff_host_rate: num  0.05 0.05 0.04 0.05 0.04 0.04 0.02 0.01 0 0 ...
 $ dst_host_serror_rate       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ dst_host_srv_serror_rate   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ dst_host_rerror_rate       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ dst_host_srv_rerror_rate   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ prediction                 : int  0 0 28 0 0 0 0 0 0 0 ...
*/

//it’s possible to extract the cluster assignment and then show statistics about the distribution of assignments
clusters <- clustering_sample["prediction"] 	//only the clustering assignmnet column (prediction)
data <- data.matrix(within(clustering_sample, rm("prediction"))) //everything but the clustering assignment
table(clusters)
/*
clusters
    0    11    17    18    23    25    28    31    33    36    40    64    79
47430     1     1     1     3   266    87    42  1140    15     2     1     1
*/

//Didn't realize we shouldn't do part over SparkR D: